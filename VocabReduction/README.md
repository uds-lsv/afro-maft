
To collect the sub-tokens for the languages you are interested in, run the command

```
bash tokenize.sh
```
You need to provide the monolingual data or the concatenation of monolingual data for several languages. This will return a pickle object containing the sub-tokens and their frequencies in the monolingual data for which you have provided. 

Then follow the instruction in the jupyter notebook on the vocabulary instrcution
